{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics and Twitter descriptions for the two artists you selected in Module 1. If the results from that pull were not to your liking, you are welcome to use the zipped data from the ‚ÄúAssignment Materials‚Äù section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e65f73",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9f064bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/landonpadgett/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcbe6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA : \n",
    "    for em in emoji.EMOJI_DATA[country] : \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "\n",
    "    # Place your Module 2 solution here\n",
    "    \n",
    "    return(0)\n",
    "\n",
    "\n",
    "    \n",
    "def contains_emoji(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    return(tokens)\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    # modify this function to return tokens\n",
    "    return(text)\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff88201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel fre to use the below cells as an example or read in the data in a way you prefer\n",
    "\n",
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/users/landonpadgett/Desktop/M1 Results/\" \n",
    "twitter_folder = \"/users/landonpadgett/Desktop/M1 Results/twitter/\"\n",
    "lyrics_folder = \"/users/landonpadgett/Desktop/M1 Results/lyrics/\"\n",
    "\n",
    "artist_files = {'cher':'cher_followers_data.txt',\n",
    "                'robyn':'robynkonichiwa_followers_data.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df415d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the artist files\n",
    "artist_files = {\n",
    "    'cher': 'cher_followers_data.txt',\n",
    "    'robyn': 'robynkonichiwa_followers_data.txt'\n",
    "}\n",
    "\n",
    "# Read the CSV file for Cher\n",
    "twitter_data = pd.read_csv(twitter_folder + artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "# Add the artist column\n",
    "twitter_data['artist'] = \"cher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966804cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the artist files\n",
    "artist_files = {\n",
    "    'cher': 'cher_followers_data.txt',\n",
    "    'robyn': 'robynkonichiwa_followers_data.txt'\n",
    "}\n",
    "\n",
    "# Read the CSV file for Cher\n",
    "twitter_data = pd.read_csv(twitter_folder + artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "# Add the artist column for Cher\n",
    "twitter_data['artist'] = \"cher\"\n",
    "\n",
    "# Read the CSV file for Robyn\n",
    "twitter_data_2 = pd.read_csv(twitter_folder + artist_files['robyn'],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "\n",
    "# Add the artist column for Robyn\n",
    "twitter_data_2['artist'] = \"robyn\"\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "twitter_data = pd.concat([twitter_data, twitter_data_2])\n",
    "\n",
    "# Delete the second dataframe to free memory\n",
    "del twitter_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "674767d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lyrics files found for cher.\n",
      "No lyrics files found for robyn.\n",
      "    screen_name          name                   id        location  \\\n",
      "0        hsmcnp  Country Girl             35152213             NaN   \n",
      "1    horrormomy          Jeny   742153090850164742           Earth   \n",
      "2  anju79990584          anju  1496463006451974150             NaN   \n",
      "3  gallionjenna             J           3366479914             NaN   \n",
      "4       bcscomm       bcscomm             83915043  Washington, DC   \n",
      "\n",
      "   followers_count  friends_count  \\\n",
      "0             1302           1014   \n",
      "1               81            514   \n",
      "2               13            140   \n",
      "3              752            556   \n",
      "4              888           2891   \n",
      "\n",
      "                                         description artist  \n",
      "0                                                NaN   cher  \n",
      "1           ùôøùöõùöòùöûùöç ùöúùöûùöôùöôùöòùöõùöùùöéùöõ ùöòùöè ùöñùöéùöúùöúùö¢ ùöãùöûùöóùöú & ùöïùöéùöêùöêùöíùöóùöêùöú   cher  \n",
      "2          163„éùÔºèÊÑõ„Åã„Å£„Å∑üíú26Ê≠≥üçí Â∑•„ÄáÂ•Ω„Åç„Å™Â•≥„ÅÆÂ≠êüíì „Éï„Ç©„É≠„Éº„Åó„Å¶„Åè„Çå„Åü„ÇâDM„Åó„Åæ„Åôüß°   cher  \n",
      "3                                                csu   cher  \n",
      "4  Writer @Washinformer @SpelmanCollege alumna #D...   cher  \n",
      "Empty DataFrame\n",
      "Columns: [line, artist]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Read Twitter data for Cher\n",
    "twitter_data = pd.read_csv(twitter_folder + artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "twitter_data['artist'] = \"cher\"\n",
    "\n",
    "# Read Twitter data for Robyn\n",
    "twitter_data_2 = pd.read_csv(twitter_folder + artist_files['robyn'],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = \"robyn\"\n",
    "\n",
    "# Concatenate Twitter data\n",
    "twitter_data = pd.concat([twitter_data, twitter_data_2])\n",
    "del twitter_data_2\n",
    "\n",
    "# Function to read all lyrics files for a given artist\n",
    "def read_lyrics(artist, folder_path):\n",
    "    lyrics_data_list = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(artist) and file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            lyrics_data = pd.read_csv(file_path, \n",
    "                                      sep=\"\\t\", \n",
    "                                      quoting=3, \n",
    "                                      header=None, \n",
    "                                      names=[\"line\"])  # Adjust as per the file structure\n",
    "            lyrics_data['artist'] = artist\n",
    "            lyrics_data_list.append(lyrics_data)\n",
    "    \n",
    "    # Check if there is data to concatenate\n",
    "    if lyrics_data_list:\n",
    "        return pd.concat(lyrics_data_list, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"No lyrics files found for {artist}.\")\n",
    "        return pd.DataFrame(columns=[\"line\", \"artist\"])  # Return an empty DataFrame with the correct columns\n",
    "\n",
    "# Read all lyrics files for Cher\n",
    "cher_lyrics_data = read_lyrics('cher', lyrics_folder)\n",
    "\n",
    "# Read all lyrics files for Robyn\n",
    "robyn_lyrics_data = read_lyrics('robyn', lyrics_folder)\n",
    "\n",
    "# Concatenate all lyrics data\n",
    "if not cher_lyrics_data.empty or not robyn_lyrics_data.empty:\n",
    "    lyrics_data = pd.concat([cher_lyrics_data, robyn_lyrics_data], ignore_index=True)\n",
    "else:\n",
    "    lyrics_data = pd.DataFrame(columns=[\"line\", \"artist\"])  # Return an empty DataFrame with the correct columns\n",
    "\n",
    "# Display final DataFrames\n",
    "print(twitter_data.head())\n",
    "print(lyrics_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords (optional)\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you. Note that we remove stopwords before removing punctuation because the stopword set includes punctuation.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c09f66ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Data Stats:\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/twitter/cher_followers_data.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/twitter/robynkonichiwa_followers_data.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/twitter/cher_followers.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/twitter/robynkonichiwa_followers.txt\n",
      "There are 58532931 tokens in the data.\n",
      "There are 12777699 unique tokens in the data.\n",
      "There are 368240283 characters in the data.\n",
      "The lexical diversity is 0.218 in the data.\n",
      "The 5 most common tokens are:\n",
      "and: 598725\n",
      "a: 409768\n",
      "the: 400525\n",
      "I: 393219\n",
      "of: 365284\n",
      "\n",
      "Lyrics Data Stats:\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_includemeout.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_electric.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_beach2k20.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_lovekills.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_timemachine.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_lovekills114524.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_givingyouback.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_noneofdem114527.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_noneofdem.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_bemine.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_fembot114519.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_shouldhaveknown.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_underneaththeheart.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_eclipse.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_robynishere.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_dontstopthemusic.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_criminalintent.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_myonlyreason.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_humanbeing.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_obaby.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_how.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_loveisfree.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_longgone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_indestructibleacousticversion.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_hangwithme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_shouldhaveknown106828.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_moonlight.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_getmyselftogether.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_universalwoman.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_bumpyride.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_sayit.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_callyourgirlfriend.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_tellyoutoday.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_whosthatgirl.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_aintnothing.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_dancingonmyown114521.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_mainthing.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_crywhenyougetolder.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_dancehallqueen114530.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_fembot.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_everagain.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_mondaymorning.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_inmyeyes.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_dancingonmyown.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_showmelove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_buffalostance.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_jackuoff.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_psycho.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_iwish.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_betweenthelines.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_wedancetothebeat114528.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_dancehallqueen.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_dontwantyouback.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_dontfuckingtellmewhattodo.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_inmyheart.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_losecontrol.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_anytimeyoulike.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_wheredidourlovego.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_justanothergirlfriend.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_cobrastyle.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_jagvetendejligrosa.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_healthylove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_doitagain.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_witheveryheartbeat.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_babyforgiveme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_ushouldknowbetter114529.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_curriculumvitae.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_monument.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_ushouldknowbetter.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_becauseitsinthemusic.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_bumlikeyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_setmefree.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_keepthisfireburning.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_indestructible.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_mytruth.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_dontfuckingtellmewhattodo114520.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_hangwithme114525.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_stars4ever.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_stillyourgirl.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_88days.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_youvegotthatsomething.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_blowmymind.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_handleme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_sendtorobinimmediately.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_hangwithmeacousticversion.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_bigcity.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_konichiwabitches.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_doyouknowwhatittakes.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_inmyeyes114532.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_gottoworkitout.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_bionicwoman.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_tomteverkstan.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_herewego.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_robotboy.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_honey.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_crashandburngirl.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_everylittlething.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_notontheinside.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_wedancetothebeat.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_doyoureallywantmeshowrespect.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_thelasttime.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_missingu.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_breakdownintermission.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/robyn/robyn_play.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_comeandstaywithme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_pirate.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_stars.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thesedays.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_lovesohigh.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_downdowndown.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_youvemademesoveryhappy.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_julie.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_reasontobelieve.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_disastercake.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_taxitaxi.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dovelamore.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_stillinlovewithyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_aliveagain.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_jolsonmedley.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mylove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_untilitstimeforyoutogo.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dontthinktwice.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_takinbackmyheart.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_reallove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_sistersofmercy.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_darklady.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_livinginahousedivided.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_youtakeitall.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_borrowedtime.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_awomansstory.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_olmanriver.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_themanthatgotaway.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ayounggirluneenfante.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_oogaboo.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thetwelfthofnever.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_therebutforfortune.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_takemeforalittlewhile.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_themanilove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mommalooksharp.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whataboutthemoonlight.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_loveenough.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_italladdsupnow.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ourladyofsanfrancisco.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_earlymorningstrangers.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_emotionalfire.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_bangbang.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_bodytobodyhearttoheart.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mainman.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_easytobehard.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ificouldturnbacktime.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_howcanyoumendabrokenheart.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_onebyone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_carnival.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_fernando.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_runaway.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thewinnertakesitall.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_iknowyoudontloveme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_runnin.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_88degrees.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thunderstorm.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_flashback.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ithrewitallaway.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_crylikeababy.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_red.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_heyjoe.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thelongandwindingroad.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_donthideyourlove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thebellsofrhymney.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mirrorimage.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_aworldwithoutheroes.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_howlonghasthisbeengoingon.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_loveoneanother.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_sos.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_willyoulovemetomorrow.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_catchthewind.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dixiegirl.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_pride.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whywasiborn.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_alfie.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_warpaintandsoftfeathers.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_womansworld.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_youbettersitdownkids.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_takeitlikeaman.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_weregonnamakeit.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_hewasbeautiful.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_shapeofthingstocome.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_strongenough.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_themusicsnogoodwithoutyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_allornothing.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_sunny.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dangeroustimes.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_willyouwaitforme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_itsamansmansmansworld.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_letthisbealessontoyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_piedpiper.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_needlesandpins.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_laplane.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_tonightillbestayingherewithyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_withorwithoutyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mamawhenmydollieshavebabies.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_halfbreed.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_walkinginmemphis.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_oneofus.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whoyougonnabelieve.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_illneverstoplovingyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thecruelwar.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_wheredoyougo.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_perfection.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_island.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_again.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mastersofwar.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_doesanybodyreallyfallinloveanymore.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_hellonwheels.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_rainrain.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_igotitbadandthataintgood.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mysongtoofargone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_notenoughloveintheworld.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whatllido.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dixie.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_skindeep.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dorightwomandorightman.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_iwillwaitforyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_homewardbound.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thebiggertheycomethehardertheyfall.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_loveisthegroove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_milord.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_iwasntready.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_silverwingsgoldenrings.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_afterall.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_iminthemiddle.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_weallsleepalone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_itsnotunusual.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_buticantloveyoumore.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_itmightaswellstaymondayfromnowon.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_justthisonetime.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ilovemakinlovetoyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_sirens.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_fireandrain.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_singforyoursupper.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_behindthedoor.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_touchandgo.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_moveme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_saveupallyourtears.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_loveme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_spring.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_nevershouldvestarted.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_waterloo.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_happywasthedaywemet.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_youhaventseenthelastofme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_itgetsmewhereiwanttogo.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_loveandunderstanding.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_shelovestohearthemusic.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_lookatme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whenyoufindoutwhereyouregoinletmeknow.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_isawamanandhedancedwithhiswife.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_heartofstone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_davidssong.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dancingqueen.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_still.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_sendthemanover.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_rescueme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_youvereallygotaholdonme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_outrageous.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dowhatyougottado.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_holysmoke.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_rockandrolldoctor.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_twopeopleclingingtoathread.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mrsoul.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thegirlfromipanema.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_walls.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thegunman.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_idonthavetosleeptodream.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_amiblue.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_canyoufool.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whenloversbecomestrangers.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_iwalkonguildedsplinters.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_houseisnotahome.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_clicksong.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_paradiseishere.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_bellbottomblues.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_likearollingstone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_walkwithme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_songforyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whenyouwalkaway.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_angelsrunning.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_loveisalonelyplacewithoutyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_yoursuntiltomorrow.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ihatetosleepalone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_superstar.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_justenoughtokeepmehanginon.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_makethemanloveme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_workinggirl.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_youngandpretty.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_misssubwayof1952.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_impossibledream.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_theshoopshoopsongitsinhiskiss.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_gitdownguitargroupie.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thethoughtoflovingyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_knockonwood.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_bangbangmybabyshotmedown.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_applesdontfallfarfromthetree.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_blowininthewind.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thesamemistake.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_games.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thesunaintgonnashineanymore.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_adifferentkindoflovesong.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_hellneverknow.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_idratherbelieveinyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_prisoner.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_gonow.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_morethanyouknow.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_melody.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_couldvebeenyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thefirsttime.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_lovehurts312103.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thepower.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ourdaywillcome.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_fernando710922.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_neverbeentospain.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_laybabylay.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_youdonthavetosayyouloveme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_itsacryinshame.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_gypsiestrampsandthieves.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_carouselman.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_chiquititaspanishversion.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_favouritescars.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_justlikejessejames.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_girldontcome.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thegreatestsongieverheard.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mammamia.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_iparalyze.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_savethechildren.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_gimmegimmegimmeamanaftermidnight.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_rudy.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_firesofeden.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_bornwiththehunger.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_cometoyourwindow.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_kisstokiss.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ifoundyoulove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_geronimoscadillac.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_believe.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_imblowinaway.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_igotosleep.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_saytheword.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_crymyselftosleep.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ibelieve.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thewayoflove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_chiquitita.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_lovepaintheresapaininmyheart.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_iwantyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_longdistanceloveaffair.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thisisasongforthelonely.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_songcalledchildren.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_welcometoburlesque.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ifeelsomethingintheairmagicintheair.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_mylove318663.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ifoundsomeone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_trainofthought.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_lovethedeviloutofya.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_boysandgirls.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_bymyself.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dannyboy.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_iwouldnttreatadogthewayyoutreatedme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_doyoubelieveinmagic.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_doievercrossyourmind.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_youknowit.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_heaintheavyhesmybrother.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dressedtokill.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_igotyoubabe.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whenthemoneysgone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thefallkurtsblues.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whentheloveisgone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_itstoolatetolovemenow.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dontcomearoundtonight.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_inforthenight.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_weallflyhome.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thetimestheyareachangin.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_letmedowneasy.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_happinessisjustathingcalledjoe.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_fittofly.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_onesmallstep.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_shoppin.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_chastitysun.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_fastcompany.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_elusivebutterfly.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_giveourloveafightinchance.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_justwhativebeenlookinfor.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_donttrytoclosearose.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_saywhatsonyourmind.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thenameofthegame.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_allireallywanttodo.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_whenlovecallsyourname.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_sittinonthedockofthebay.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ifiknewthen.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thegreatestthing.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_allbecauseofyou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_onehonestman.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_loversforever.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_lovehurts.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_forwhatitsworth.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_holdinoutforlove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_takemehome.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thebookoflove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_chastityssongbandofthieves.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_startingover.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_shadowdreamsong.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_wasntitgood.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_youwouldntknowlove.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_thisgodforsakenday.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_time.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_iwalkalone.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_lietome.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_backonthestreetagain.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_loveonarooftop.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_hardenoughgettingoveryou.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_takeitfromtheboys.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_dreambaby.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_pleasedonttellme.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_ihopeyoufindit.txt\n",
      "Processing file: /users/landonpadgett/Desktop/M1 Results/lyrics/cher/cher_classified1a.txt\n",
      "There are 99415 tokens in the data.\n",
      "There are 7674 unique tokens in the data.\n",
      "There are 391566 characters in the data.\n",
      "The lexical diversity is 0.077 in the data.\n",
      "The 5 most common tokens are:\n",
      "I: 3480\n",
      "you: 3427\n",
      "the: 2942\n",
      "to: 2174\n",
      "me: 1881\n"
     ]
    }
   ],
   "source": [
    "data_location = \"/users/landonpadgett/Desktop/M1 Results/\" \n",
    "twitter_folder = \"/users/landonpadgett/Desktop/M1 Results/twitter/\"\n",
    "lyrics_folder = \"/users/landonpadgett/Desktop/M1 Results/lyrics/\"\n",
    "\n",
    "artist_files = {'cher':'cher_followers_data.txt',\n",
    "                'robyn':'robynkonichiwa_followers_data.txt'}\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Reads the content of a file and returns it as a string.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Processes all .txt files in a folder and its subfolders.\n",
    "    Returns tokenized text as a list of words.\n",
    "    \"\"\"\n",
    "    all_data = \"\"\n",
    "    # Walk through all subfolders and files\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):  # Only process .txt files\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")  # Debugging output\n",
    "                all_data += read_file(file_path) + \" \"\n",
    "    return all_data.split()  # Tokenize by splitting on whitespace\n",
    "\n",
    "def descriptive_stats(tokens, num_tokens=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "    number of characters, lexical diversity, and num_tokens most common tokens. \n",
    "    Return a list with the number of tokens, number of unique tokens, \n",
    "    lexical diversity, and number of characters. \n",
    "    \"\"\"\n",
    "    # Total number of tokens\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    # Number of unique tokens\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    \n",
    "    # Lexical diversity\n",
    "    lexical_diversity = num_unique_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    # Number of characters\n",
    "    num_characters = sum(len(token) for token in tokens)\n",
    "    \n",
    "    # Find the most common tokens\n",
    "    token_counts = Counter(tokens)\n",
    "    most_common_tokens = token_counts.most_common(num_tokens)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"There are {total_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "        print(f\"The {num_tokens} most common tokens are:\")\n",
    "        for token, count in most_common_tokens:\n",
    "            print(f\"{token}: {count}\")\n",
    "    \n",
    "    return [total_tokens, num_unique_tokens, lexical_diversity, num_characters]\n",
    "\n",
    "# Paths to Twitter and Lyrics folders\n",
    "data_location = \"/users/landonpadgett/Desktop/M1 Results/\"\n",
    "twitter_folder = os.path.join(data_location, \"twitter/\")\n",
    "lyrics_folder = os.path.join(data_location, \"lyrics/\")\n",
    "\n",
    "# Process and analyze Twitter data\n",
    "print(\"Twitter Data Stats:\")\n",
    "twitter_tokens = process_folder(twitter_folder)\n",
    "twitter_stats = descriptive_stats(twitter_tokens, verbose=True)\n",
    "\n",
    "# Process and analyze Lyrics data (including subfolders like cher/ and robyn/)\n",
    "print(\"\\nLyrics Data Stats:\")\n",
    "lyrics_tokens = process_folder(lyrics_folder)\n",
    "lyrics_stats = descriptive_stats(lyrics_tokens, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: \n",
    "An area of improvement for the tokenization process could be more effective handling of hashtags and emojis. Instead of treating them as single tokens, we could split hashtags into meaningful words and convert emojis into descriptive words (e.g., üòä ‚Üí \"smiley_face\"). This would enhance feature extraction and improve the contextual understanding of the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc25e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Descriptive Statistics Between Twitter and Lyrics Data:\n",
      "                 Metric  Twitter Data    Lyrics Data    Difference\n",
      "0          Total Tokens  5.853293e+07   99415.000000  5.843352e+07\n",
      "1         Unique Tokens  1.277770e+07    7674.000000  1.277002e+07\n",
      "2     Lexical Diversity  2.182993e-01       0.077192  1.411077e-01\n",
      "3  Number of Characters  3.682403e+08  391566.000000  3.678487e+08\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Total Tokens\", \"Unique Tokens\", \"Lexical Diversity\", \"Number of Characters\"],\n",
    "    \"Twitter Data\": twitter_stats,\n",
    "    \"Lyrics Data\": lyrics_stats\n",
    "})\n",
    "\n",
    "# Calculate differences between the two sets\n",
    "comparison_df[\"Difference\"] = comparison_df[\"Twitter Data\"] - comparison_df[\"Lyrics Data\"]\n",
    "\n",
    "# Display the comparison DataFrame\n",
    "print(\"\\nComparison of Descriptive Statistics Between Twitter and Lyrics Data:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: The Twitter data contains significantly more total tokens (approximately 58.5 million) compared to the Lyrics data, which has only around 99,415 tokens. This substantial difference suggests that the Twitter dataset is much larger, likely due to the sheer volume of user-generated posts compared to a limited collection of song lyrics. Additionally, the Twitter data has a much higher number of unique tokens (about 12.8 million) compared to the Lyrics data's 7,674 unique tokens, reflecting the diverse vocabulary and variety of expressions found on the social media platform. The lexical diversity of Twitter data (0.218) is also notably higher than that of the Lyrics data (0.077), indicating that the Twitter dataset has a wider range of distinct words relative to its total word count. Finally, the total number of characters is dramatically larger in the Twitter data, which aligns with the overall size and richness of the dataset compared to the more repetitive and structured nature of song lyrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce72f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 tokens unique to Twitter data based on concentration ratio:\n",
      "Token: i, Concentration Ratio: 30.82\n",
      "Token: 10, Concentration Ratio: 19.27\n",
      "Token: 22, Concentration Ratio: 14.47\n",
      "Token: Music, Concentration Ratio: 9.78\n",
      "Token: que, Concentration Ratio: 7.29\n",
      "Token: family, Concentration Ratio: 5.75\n",
      "Token: Follow, Concentration Ratio: 4.35\n",
      "Token: James, Concentration Ratio: 4.24\n",
      "Token: follow, Concentration Ratio: 4.24\n",
      "Token: lover, Concentration Ratio: 3.99\n",
      "\n",
      "Top 10 tokens unique to Lyrics data based on concentration ratio:\n",
      "Token: cryin', Concentration Ratio: 3414.89\n",
      "Token: Ooh,, Concentration Ratio: 3336.38\n",
      "Token: digi, Concentration Ratio: 1712.80\n",
      "Token: (Let's, Concentration Ratio: 1345.77\n",
      "Token: Ohh,, Concentration Ratio: 1177.55\n",
      "Token: Taxi,, Concentration Ratio: 981.29\n",
      "Token: splinters, Concentration Ratio: 883.16\n",
      "Token: conceal, Concentration Ratio: 856.40\n",
      "Token: indestructible, Concentration Ratio: 856.40\n",
      "Token: tellin', Concentration Ratio: 824.28\n"
     ]
    }
   ],
   "source": [
    "def calculate_concentration(tokens):\n",
    "    \"\"\"\n",
    "    Calculate the concentration of each token in the corpus.\n",
    "    Returns a dictionary with tokens as keys and their concentration as values.\n",
    "    \"\"\"\n",
    "    total_tokens = len(tokens)\n",
    "    token_counts = Counter(tokens)\n",
    "    concentration = {token: count / total_tokens for token, count in token_counts.items()}\n",
    "    return concentration\n",
    "\n",
    "def calculate_concentration_ratios(conc1, conc2, min_count, token_counts1, token_counts2):\n",
    "    \"\"\"\n",
    "    Calculate the concentration ratios of tokens between two corpora.\n",
    "    Returns a dictionary with tokens as keys and their concentration ratio as values.\n",
    "    \"\"\"\n",
    "    ratios = {}\n",
    "    for token in conc1:\n",
    "        # Check if the token appears at least min_count times in both corpora\n",
    "        if token_counts1.get(token, 0) >= min_count and token_counts2.get(token, 0) >= min_count:\n",
    "            # Calculate the concentration ratio\n",
    "            ratio = conc1[token] / conc2[token] if conc2[token] > 0 else float('inf')\n",
    "            ratios[token] = ratio\n",
    "    return ratios\n",
    "\n",
    "def top_tokens_by_ratio(conc1, conc2, token_counts1, token_counts2, min_count, top_n=10):\n",
    "    \"\"\"\n",
    "    Find the top `top_n` tokens with the highest concentration ratio for each corpus.\n",
    "    \"\"\"\n",
    "    # Calculate concentration ratios\n",
    "    ratios1_to_2 = calculate_concentration_ratios(conc1, conc2, min_count, token_counts1, token_counts2)\n",
    "    ratios2_to_1 = calculate_concentration_ratios(conc2, conc1, min_count, token_counts2, token_counts1)\n",
    "    \n",
    "    # Sort tokens by concentration ratio and get the top_n\n",
    "    top_tokens_corp1 = sorted(ratios1_to_2.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_tokens_corp2 = sorted(ratios2_to_1.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    return top_tokens_corp1, top_tokens_corp2\n",
    "\n",
    "# Calculate token concentrations for both corpora\n",
    "twitter_concentration = calculate_concentration(twitter_tokens)\n",
    "lyrics_concentration = calculate_concentration(lyrics_tokens)\n",
    "\n",
    "# Count the token frequencies in both corpora\n",
    "twitter_token_counts = Counter(twitter_tokens)\n",
    "lyrics_token_counts = Counter(lyrics_tokens)\n",
    "\n",
    "# Define minimum token appearance count cutoff\n",
    "min_count = 5  # Adjust this based on the size of your corpora\n",
    "\n",
    "# Get the top 10 tokens for each corpus based on concentration ratios\n",
    "top_twitter_tokens, top_lyrics_tokens = top_tokens_by_ratio(\n",
    "    twitter_concentration, \n",
    "    lyrics_concentration, \n",
    "    twitter_token_counts, \n",
    "    lyrics_token_counts, \n",
    "    min_count, \n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 tokens unique to Twitter data based on concentration ratio:\")\n",
    "for token, ratio in top_twitter_tokens:\n",
    "    print(f\"Token: {token}, Concentration Ratio: {ratio:.2f}\")\n",
    "\n",
    "print(\"\\nTop 10 tokens unique to Lyrics data based on concentration ratio:\")\n",
    "for token, ratio in top_lyrics_tokens:\n",
    "    print(f\"Token: {token}, Concentration Ratio: {ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: The Twitter data shows a lot of everyday language, with personal pronouns like \"I\" and numbers being common, along with words like \"family\" and \"follow,\" which are typical for social interactions and self-expression on social media. In contrast, the lyrics data is full of expressive and artistic words like \"cryin',\" \"Ooh,\" and \"indestructible,\" which you‚Äôd expect in a more poetic context. The high concentration ratios for these lyrical tokens suggest they‚Äôre used in ways that are much more unique and creative compared to the more casual, conversational style of Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "786b2af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/landonpadgett/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/landonpadgett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /users/landonpadgett/Desktop/M1 Results/nltk_data/...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/landonpadgett/Desktop/M1 Results/nltk_data/...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/landonpadgett/nltk_data'\n    - '/Users/landonpadgett/miniconda3/envs/myenv/nltk_data'\n    - '/Users/landonpadgett/miniconda3/envs/myenv/share/nltk_data'\n    - '/Users/landonpadgett/miniconda3/envs/myenv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/users/landonpadgett/Desktop/M1 Results/nltk_data/'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Assuming `lyrics_data` and `twitter_data` are already defined and contain the data you provided earlier\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[1;32m     58\u001b[0m lyrics_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lyrics_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m---> 59\u001b[0m twitter_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m twitter_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Calculate word frequencies for both datasets\u001b[39;00m\n\u001b[1;32m     62\u001b[0m lyrics_word_freq \u001b[38;5;241m=\u001b[39m calculate_word_frequencies(lyrics_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[62], line 22\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# Convert to lowercase\u001b[39;00m\n\u001b[1;32m     21\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, string\u001b[38;5;241m.\u001b[39mpunctuation))  \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)  \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[1;32m     23\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/landonpadgett/nltk_data'\n    - '/Users/landonpadgett/miniconda3/envs/myenv/nltk_data'\n    - '/Users/landonpadgett/miniconda3/envs/myenv/share/nltk_data'\n    - '/Users/landonpadgett/miniconda3/envs/myenv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/users/landonpadgett/Desktop/M1 Results/nltk_data/'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk_data_path = \"/users/landonpadgett/Desktop/M1 Results/nltk_data/\"\n",
    "\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Download the 'punkt' tokenizer to the specified directory\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Utility function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []  # Return an empty list for non-string values (e.g., NaN, float)\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# Function to calculate word frequencies\n",
    "def calculate_word_frequencies(tokens_list):\n",
    "    all_tokens = [word for tokens in tokens_list for word in tokens]  # Flatten list of lists\n",
    "    return Counter(all_tokens)\n",
    "\n",
    "# Function to generate word cloud\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color=\"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # Convert data frame into dict if it's a Series\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # Filter stopwords in frequency counter if provided\n",
    "    if stopwords is not None:\n",
    "        counter = {token: freq for (token, freq) in counter.items() \n",
    "                   if token not in stopwords}\n",
    "    \n",
    "    wc.generate_from_frequencies(counter)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))  # Add figure size to display it clearly\n",
    "    plt.title(title) \n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()  # This is necessary to display the plot\n",
    "\n",
    "# Assuming `lyrics_data` and `twitter_data` are already defined and contain the data you provided earlier\n",
    "# Preprocess the data\n",
    "lyrics_data['tokens'] = lyrics_data['line'].apply(preprocess_text)\n",
    "twitter_data['tokens'] = twitter_data['description'].apply(preprocess_text)\n",
    "\n",
    "# Calculate word frequencies for both datasets\n",
    "lyrics_word_freq = calculate_word_frequencies(lyrics_data['tokens'])\n",
    "twitter_word_freq = calculate_word_frequencies(twitter_data['tokens'])\n",
    "\n",
    "# Generate word clouds for both datasets\n",
    "wordcloud(lyrics_word_freq, title=\"Lyrics Data Word Cloud\")\n",
    "wordcloud(twitter_word_freq, title=\"Twitter Data Word Cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
